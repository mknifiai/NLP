{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c6415d6-9699-478c-a4cc-a0f9671af12b",
   "metadata": {},
   "source": [
    "## How Does Natural Language Processing (NLP) Work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab620adc-b129-49bd-a262-69d2ec7d4f2d",
   "metadata": {},
   "source": [
    "NLP models work by finding relationships between the constituent parts of language — for example, the letters, words, and sentences found in a text dataset. \n",
    "\n",
    "NLP architectures use various methods for data preprocessing, feature extraction, and modeling. Some of these processes are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a596442b-8787-4089-b030-47aa0c858fad",
   "metadata": {},
   "source": [
    "- Data preprocessing: Before a model processes text for a specific task, the text often needs to be preprocessed to improve model performance or to turn words and characters into a format the model can understand. Various techniques may be used in this data preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae931ea-422a-49e5-a279-b0939eef06e8",
   "metadata": {},
   "source": [
    "#### Sentence segmentation & Word tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cc99be-a650-41fb-802e-db371bcb8155",
   "metadata": {},
   "source": [
    "    - Sentence segmentation breaks a large piece of text into linguistically meaningful sentence units. This is obvious in languages like English, where the end of a sentence is marked by a period, but it is still not trivial. A period can be used to mark an abbreviation as well as to terminate a sentence, and in this case, the period should be part of the abbreviation token itself. The process becomes even more complex in languages, such as ancient Chinese, that don’t have a delimiter that marks the end of a sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adfb63c-9aae-4d7d-b7c3-a9697b8f9707",
   "metadata": {},
   "source": [
    "    -Tokenization splits text into individual words and word fragments. The result generally consists of a word index and tokenized text in which words may be represented as numerical tokens for use in various deep learning methods. A method that instructs language models to ignore unimportant tokens can improve efficiency.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "627decb4-f329-431e-8c88-3c7f822c3de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"tokenizer.png\" width=\"500\" height=\"350\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url=\"tokenizer.png\", width=500, height=350)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7343eade-39db-4bb9-8029-bc5b6f302ffb",
   "metadata": {},
   "source": [
    "#### Stemming, lemmatization, and Stop word removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416b5383-a72f-4ae5-9363-1d805d29ecba",
   "metadata": {},
   "source": [
    "        - Stemming and lemmatization: Stemming is an informal process of converting words to their base forms using heuristic rules. For example, “university,” “universities,” and “university’s” might all be mapped to the base univers. (One limitation in this approach is that “universe” may also be mapped to univers, even though universe and university don’t have a close semantic relationship.) \n",
    "       \n",
    "       - Lemmatization is a more formal way to find roots by analyzing a word’s morphology using vocabulary from a dictionary. Stemming and lemmatization are provided by libraries like spaCy and NLTK. \n",
    "\n",
    "\n",
    "        - Stop word removal aims to remove the most commonly occurring words that don’t add much information to the text. For example, “the,” “a,” “an,” and so on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3e5c833-2a90-4a57-aeb3-26e7b8d7468a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"stemlem.png\" width=\"500\" height=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"stemlem.png\", width=500, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed58eb98-0fb8-4124-ac2d-90311df60587",
   "metadata": {},
   "source": [
    "#### Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f7b3a1-49c2-4a61-a8d8-a788d28f8786",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Most conventional machine-learning techniques work on the features – generally numbers that describe a document in relation to the corpus that contains it. \n",
    "\n",
    "Text representation are classified into four categories:\n",
    "\n",
    "- Basic vectorization approaches\n",
    "- Distributed representations\n",
    "- Universal language representation\n",
    "- Handcrafted features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48852c6-974c-49df-942c-a5b8c9a2cb60",
   "metadata": {},
   "source": [
    "##### Basic Vectorization\n",
    "\n",
    "- One-Hot Encoding: In one-hot encoding, each word w in the corpus vocabulary is given a unique integer ID $w_{id}$ that is between 1 and |V|, where V is the set of the corpus vocabulary. Each word is then represented by a V-dimensional binary vector of 0s and 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf9f1ab0-1617-4b63-a6fb-9b1af7416645",
   "metadata": {},
   "outputs": [],
   "source": [
    "Output: [[0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf243f0f-454b-4344-a6bd-bfbd8995eb8e",
   "metadata": {},
   "source": [
    "Cons:\n",
    "   - The size of a one-hot vector is directly proportional to size of the vocabulary, and most real-world corpora have large vocabularies.\n",
    "   - This representation does not give a fixed-length representation for text\n",
    "   - It treats words as atomic units and has no notion of (dis)similarity between words.\n",
    "   - It cannot handle the out of vocabulary (OOV) problem.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39010ab0-ed71-4439-86e1-49b0441ac74c",
   "metadata": {},
   "source": [
    "##### Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30056fb-1ed0-43ad-bed4-6f4e8b0ac27c",
   "metadata": {
    "tags": []
   },
   "source": [
    "   - Bag-of-Words: Bag-of-Words counts the number of times each word or n-gram (combination of n words) appears in a document. For example, below, the Bag-of-Words model creates a numerical representation of the dataset based on how many of each word in the word_index occur in the document.\n",
    "   \n",
    "       - The size of the vector increases with the size of the vocabulary.\n",
    "       - It does not capture the similarity between different words that mean the same thing.\n",
    "           - Bag of N-grams: captures some context and word-order information in the form of n-grams\n",
    "       - This representation does not have any way to handle out of vocabulary words.\n",
    "       - word order information is lost in this representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5fa399a-696f-46fc-94c6-4601cdcf0f97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"bow.png\" width=\"500\" height=\"350\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"bow.png\", width=500, height=350)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e89d1d-c93e-486d-b9b1-e15d3e06dcc5",
   "metadata": {},
   "source": [
    "##### Term frequency–inverse document frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3961a4-9d07-4021-ac6d-8c6eaf29a99f",
   "metadata": {
    "tags": []
   },
   "source": [
    "   - TF-IDF: In Bag-of-Words, we count the occurrence of each word or n-gram in a document. In contrast, with TF-IDF, we weight each word by its importance. To evaluate a word’s significance, we consider two things:\n",
    "\n",
    "        - Term Frequency: How important is the word in the document?\n",
    "\n",
    "TF(word in a document)= Number of occurrences of that word in document / Number of words in document\n",
    "\n",
    "    - Inverse Document Frequency: How important is the term in the whole corpus?\n",
    "\n",
    "IDF(word in a corpus)=log(number of documents in the corpus / number of documents that include the word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258f359d-7263-4901-8cae-8283f8356017",
   "metadata": {
    "tags": []
   },
   "source": [
    "A word is important if it occurs many times in a document. But that creates a problem. Words like “a” and “the” appear often. And as such, their TF score will always be high. We resolve this issue by using Inverse Document Frequency, which is high if the word is rare and low if the word is common across the corpus. The TF-IDF score of a term is the product of TF and IDF.\n",
    "\n",
    "The intuition behind TF-IDF is as follows: if a word $w$ appears many times in a document $d_{i}$ but does not occur much in the rest of the documents $d_{j}$ in the corpus, then the word $w$ must be of great importance to the document $d_{i}$. The importance of $w$ should increase in proportion to its frequency in $d_{i}$, but at the same time, its importance should decrease in proportion to the word’s frequency in other documents $d_{j}$ in the corpus.\n",
    "\n",
    "This method still suffers from the curse of high dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a015712-ca69-4f0b-9365-393032008494",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"tf.png\" width=\"500\" height=\"350\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"tf.png\", width=500, height=350)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900f6776-310a-4933-b322-94a345757469",
   "metadata": {},
   "source": [
    "If we look back at all the representation schemes we’ve discussed so far, we notice three fundamental drawbacks:\n",
    "- They’re discrete representations—i.e., they treat language units (words, n-grams, etc.) as atomic units. This discreteness hampers their ability to capture relationships between words.\n",
    "- The feature vectors are sparse and high-dimensional representations. The dimensionality increases with the size of the vocabulary, with most values being zero for any vector. This hampers learning capability. Further, high-dimensionality representation makes them computationally inefficient.\n",
    "- They cannot handle OOV words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebc2b66-0450-4f8d-b42a-70e8d92d199e",
   "metadata": {},
   "source": [
    "#### Distributed Representations\n",
    "\n",
    "To overcome these limitations, methods to learn low-dimensional representations were devised.\n",
    "\n",
    "Distributional similarity: \n",
    "This is the idea that the meaning of a word can be understood from the context in which the word appears. For example: “NLP rocks.” The literal meaning of the word “rocks” is “stones,” but from the context, it’s used to refer to something good and fashionable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9891026a-5d9e-44e5-9274-105e85e2027b",
   "metadata": {},
   "source": [
    "#### Word Embeddings\n",
    "\n",
    "The neural network–based word representation model known as “Word2vec,” based on “distributional similarity,” can capture word analogy relationships such as:\n",
    "\n",
    "King – Man + Woman ≈ Queen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa3fb12-66a9-486d-af8e-8171ecf864f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "It comes in two variations: \n",
    "   - Skip-Gram, in which we try to predict surrounding words given a target word\n",
    "   - Continuous Bag-of-Words (CBOW), which tries to predict the target word from surrounding words. \n",
    "   \n",
    "After discarding the final layer after training, these models take a word as input and output a word embedding that can be used as an input to many NLP tasks. Embeddings from Word2Vec capture context. If particular words appear in similar contexts, their embeddings will be similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb510f4e-a87a-47a0-9ef8-b29db4be7a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"CBOW.png\" width=\"500\" height=\"350\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"CBOW.png\", width=500, height=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bea2da84-1196-4ed6-b6b0-cda02931902a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"pCBOW.png\" width=\"500\" height=\"350\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"pCBOW.png\", width=500, height=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb2d4003-0747-4d0a-ab45-92baad2ada4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"nCBOW.png\" width=\"500\" height=\"350\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"nCBOW.png\", width=500, height=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e673d5ce-c07c-4b74-9d56-1c834d2b2e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"SG.png\" width=\"500\" height=\"350\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"SG.png\", width=500, height=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95f78351-de74-423a-b723-806bfcad00a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"pSG.png\" width=\"500\" height=\"350\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"pSG.png\", width=500, height=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74f19b66-18c7-4a2e-877d-7ebb518ae98b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"nSG.png\" width=\"500\" height=\"350\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"nSG.png\", width=500, height=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaef91d-0ea8-4091-a813-e4c078ba42e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07dae769-62a0-4ba3-a08e-6d7a0c26c3f9",
   "metadata": {},
   "source": [
    "\n",
    "GLoVE is similar to Word2Vec as it also learns word embeddings, but it does so by using matrix factorization techniques rather than neural learning. The GLoVE model builds a matrix based on the global word-to-word co-occurrence counts. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a82abf-6d15-408e-b5d9-3cc52d7daa35",
   "metadata": {},
   "source": [
    "It’s not necessary to train your own embeddings, and using pre-trained word embeddings often suffices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f89ac83-ea45-4421-b1a1-7a888f10fc33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
